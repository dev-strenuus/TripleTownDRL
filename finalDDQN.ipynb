{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1906</td><td>application_1573690044319_0875</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://hadoop33:8088/proxy/application_1573690044319_0875/\">Link</a></td><td><a target=\"_blank\" href=\"http://hadoop24:8042/node/containerlogs/container_e15_1573690044319_0875_01_000001/demo_spark_scotti01__scotti00\">Link</a></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "def keras_experiment():\n",
    "    from hops import hdfs\n",
    "    from tensorflow.keras.preprocessing import sequence\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Embedding, TimeDistributed, Dense, RepeatVector,\\\n",
    "                             Activation, Flatten, Reshape, concatenate, Dropout, BatchNormalization, Conv2D, MaxPooling2D\n",
    "    from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "    from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "    from tensorflow.keras.preprocessing import image\n",
    "    from tensorflow.keras.models import Model\n",
    "    from tensorflow.keras import Input, layers, optimizers\n",
    "    from tensorflow.keras.initializers import Constant\n",
    "    from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "    from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "    from tensorflow.keras.regularizers import l1, l2\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    from tensorflow.keras.losses import MSE\n",
    "    import tensorflow as tf\n",
    "    import numpy as np\n",
    "    import random\n",
    "    \n",
    "    \n",
    "    class Environment:\n",
    "\n",
    "        def reset(self):\n",
    "            return\n",
    "\n",
    "        def step(self, action_type, parameters):\n",
    "            return\n",
    "    \n",
    "    class Env0(Environment):\n",
    "        def __init__(self):\n",
    "            self.grid_size = 6\n",
    "            self.grid = np.zeros((self.grid_size, self.grid_size), dtype=int)\n",
    "            self.triple_merge_mapping = {1: 2, 2: 3, 3: 4, 4: 5, 5: 6, 6: 7, 7: 8}\n",
    "            #self.points = {1: 0.1, 2: 0.2, 3: 0.3, 4: 0.4, 5: 0.5, 6: 0.6, 7: 0.7,8: 0.8}  \n",
    "            #self.points = {1: 5, 2: 20, 3: 100, 4: 1000, 5: 2000, 6: 3000, 7: 4000, 8: 5000} # points are realistic only up to 4 for the moment\n",
    "            self.points = {1: 1, 2: 2, 3: 4, 4: 8, 5: 16, 6: 32, 7: 64, 8: 128}\n",
    "            self.num_tiles = len(self.points) + 1\n",
    "            self.cum_points = 0\n",
    "            self.current_tile_type = 0\n",
    "\n",
    "        def is_grid_full(self):\n",
    "            if np.count_nonzero(self.grid) == self.grid_size**2:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        def reset(self):\n",
    "            #self.grid = np.random.choice(6, self.grid_size**2, p=[0.5, 0.2, 0.1, 0.1, 0.05, 0.05]).reshape((self.grid_size, self.grid_size))\n",
    "            self.grid = np.zeros((self.grid_size, self.grid_size), dtype=int)\n",
    "            self.cum_points = 0\n",
    "            self.generate_tile()\n",
    "            return self.grid, self.current_tile_type\n",
    "\n",
    "        def get_mapping(self, num_tiles, tile_type):\n",
    "            return self.triple_merge_mapping[tile_type]\n",
    "\n",
    "        def generate_tile(self):\n",
    "            \"\"\"\n",
    "            0 -> nothing, 1 -> grass, 2 -> bush, 3 -> tree, 4 -> hut, 5 -> house, 6 -> mansion, 7 -> castle, 8 -> floating mansion, 9 -> triple castle\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            self.current_tile_type = np.random.choice([1, 2, 3, 4], 100, p=[0.6, 0.25, 0.10, 0.05])[0]\n",
    "\n",
    "        def dfs(self, tile, x, y, visited, cleaning=False):\n",
    "            if x == 0 and y == 0:\n",
    "                return 0\n",
    "            if tile == 0:\n",
    "                return 0\n",
    "            if x < 0 or x >= self.grid_size or y < 0 or y >= self.grid_size or visited[x][y] == 1:\n",
    "                return 0\n",
    "            visited[x][y] = 1\n",
    "            if self.grid[x][y] != tile:\n",
    "                return 0\n",
    "            if cleaning:\n",
    "                self.grid[x][y] = 0\n",
    "            return 1 + self.dfs(tile, x - 1, y, visited, cleaning) + self.dfs(tile, x + 1, y, visited, cleaning) + self.dfs(tile, x, y - 1,\n",
    "                                                                                                        visited, cleaning) + self.dfs(\n",
    "                tile, x, y + 1, visited, cleaning)\n",
    "\n",
    "        def check_merge(self, x, y, tile_type):\n",
    "            res = self.dfs(tile_type, x, y, np.zeros(self.grid.shape))\n",
    "            if res >= 3 and tile_type != 8:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        def place_in_grid(self, tile, x, y):\n",
    "            if self.grid[x][y] != 0:\n",
    "                return None\n",
    "            tile_type = tile\n",
    "            tot_points = 0\n",
    "            self.grid[x][y] = tile_type\n",
    "            while self.check_merge(x, y, tile_type):\n",
    "                num_tiles = self.dfs(tile_type, x, y, np.zeros(self.grid.shape), True)\n",
    "                tile_type = self.get_mapping(num_tiles, tile_type)\n",
    "                tot_points += self.points[tile_type]\n",
    "                self.grid[x][y] = tile_type\n",
    "\n",
    "            if tot_points == 0:\n",
    "                return self.points[self.current_tile_type]\n",
    "            else:\n",
    "                return tot_points\n",
    "\n",
    "        def place_in_storehouse(self):\n",
    "            if self.grid[0][0] != 0:\n",
    "                return None\n",
    "            self.grid[0][0] = self.current_tile_type\n",
    "            return 0.0001\n",
    "\n",
    "        def place_from_storehouse(self, x, y):\n",
    "            if self.grid[0][0] == 0:\n",
    "                return None\n",
    "            else:\n",
    "                tile = self.grid[0][0]\n",
    "                res = self.place_in_grid(tile, x, y)\n",
    "                if res is None:\n",
    "                    return None\n",
    "                else:\n",
    "                    self.grid[0][0] = 0\n",
    "                    return res\n",
    "\n",
    "        def step(self, action_type, parameters):\n",
    "            \"\"\"\n",
    "            :param action_type:\n",
    "            :param parameters: parameters[0] = x and parameters[1] = y\n",
    "            :return:\n",
    "            \"\"\"\n",
    "            if self.is_grid_full():\n",
    "                return -1, [self.grid, 0]\n",
    "\n",
    "            res = None\n",
    "            if action_type == \"place_current\":\n",
    "                if parameters[0] == 0 and parameters[1] == 0:\n",
    "                    res = self.place_in_storehouse()\n",
    "                else:\n",
    "                    res = self.place_in_grid(self.current_tile_type, parameters[0], parameters[1])\n",
    "            elif action_type == \"place_from_storehouse\":\n",
    "                res = self.place_from_storehouse(parameters[0], parameters[1])\n",
    "\n",
    "            if res is None:\n",
    "                return 0, [self.grid, self.current_tile_type]\n",
    "            else:\n",
    "                self.generate_tile()\n",
    "                return res, [self.grid, self.current_tile_type]\n",
    "            \n",
    "    class Learner:\n",
    "\n",
    "        def __init__(self, env):\n",
    "            self.env = env\n",
    "            self.grid, self.current_tile = self.env.reset()\n",
    "            self.cum_reward = 0\n",
    "\n",
    "        def print_grid(self):\n",
    "            print(\"Current tile: \" + str(self.current_tile))\n",
    "            for i in range(self.grid.shape[0]):\n",
    "                for j in range(self.grid.shape[1]):\n",
    "                    print(self.grid[i][j], end = ' ')\n",
    "                print(\"\\n\")\n",
    "\n",
    "        def select_action(self):\n",
    "            return\n",
    "        \n",
    "    class RandomLearner(Learner):\n",
    "\n",
    "        def __init__(self, env):\n",
    "            super().__init__(env)\n",
    "\n",
    "        def select_action(self):\n",
    "            x = np.random.randint(0, 6)\n",
    "            y = np.random.randint(0, 6)\n",
    "            while self.grid[x][y] != 0:\n",
    "                x = np.random.randint(0, 6)\n",
    "                y = np.random.randint(0, 6)\n",
    "            if np.random.uniform() > 0.1:\n",
    "                return x*6+y\n",
    "            else:\n",
    "                return x*6+y+36\n",
    "            \n",
    "            \n",
    "    class GreedyLearner(RandomLearner):\n",
    "\n",
    "        def __init__(self, env):\n",
    "            super().__init__(env)\n",
    "\n",
    "\n",
    "        def select_action(self):\n",
    "            shuffle_x = np.arange(self.grid.shape[0])\n",
    "            shuffle_y = np.arange(self.grid.shape[1])\n",
    "            np.random.shuffle(shuffle_x)\n",
    "            np.random.shuffle(shuffle_y)\n",
    "            for x in shuffle_x:\n",
    "                for y in shuffle_y:\n",
    "                    if self.grid[x][y] == 0 and self.env.check_merge(self.current_tile, x, y):\n",
    "                        return x*6+y\n",
    "\n",
    "            if self.grid[0][0] != 0:\n",
    "                for x in shuffle_x:\n",
    "                    for y in shuffle_y:\n",
    "                        if self.grid[x][y] == 0 and self.env.check_merge(self.grid[0][0], x, y):\n",
    "                            return x*6+y+36\n",
    "            for x in shuffle_x:\n",
    "                for y in shuffle_y:\n",
    "                    if (x>0 or y>0) and self.grid[x][y] == self.current_tile:\n",
    "                        if x-1 >= 0 and self.grid[x-1][y] == 0 and (x-1>0 or y>0):\n",
    "                            return (x-1)*6+y\n",
    "                        if x+1 < self.grid.shape[0] and self.grid[x+1][y] == 0:\n",
    "                            return (x+1)*6+y\n",
    "                        if y-1 >= 0 and self.grid[x][y-1] == 0 and (x>0 or y-1>0):\n",
    "                            return x*6+y-1\n",
    "                        if y+1 < self.grid.shape[1] and self.grid[x][y+1] == 0:\n",
    "                            return x*6+y+1\n",
    "\n",
    "            return super().select_action()\n",
    "            \n",
    "    \n",
    "        \n",
    "    class ReplayBuffer:\n",
    "\n",
    "        def __init__(self, buffer_size):\n",
    "            self.buffer_size = buffer_size\n",
    "            self.buffer = []\n",
    "\n",
    "        def add(self, s, a, r, s2):\n",
    "            # s represents current state, a is action,\n",
    "            # r is reward, s2 is next state\n",
    "            experience = np.array([s, a, r, s2])\n",
    "            self.buffer.append(experience)\n",
    "            if self.size() > self.buffer_size:\n",
    "                self.buffer.pop(0)\n",
    "\n",
    "        def size(self):\n",
    "            return len(self.buffer)\n",
    "\n",
    "        def sample(self, batch_size):\n",
    "\n",
    "            batch = []\n",
    "\n",
    "            if self.size() < batch_size:\n",
    "                batch = random.sample(self.buffer, self.size())\n",
    "            else:\n",
    "                batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "            s_batch, a_batch, r_batch, s2_batch = np.array(batch).T\n",
    "\n",
    "            return s_batch, a_batch, r_batch, s2_batch\n",
    "\n",
    "        def clear(self):\n",
    "            self.buffer.clear()\n",
    "        \n",
    "    TAU = 1\n",
    "    DECAY_RATE = 0.9\n",
    "    NUM_ACTIONS = 72 #if action a < 36, place the new tile in pos (a/6, a%6), else place tile in storehouse in position ((a-36)/6, (a-36)%6)\n",
    "\n",
    "    class DQLNetwork():\n",
    "        def __init__(self, num_tiles):\n",
    "            self.num_tiles = num_tiles\n",
    "            self.construct_q_network()\n",
    "\n",
    "        def define_model(self):\n",
    "            input_layer1 = Input(shape=(6, 6, 9))\n",
    "            input_layer2 = Input(shape=(9))\n",
    "            dense0 = Dense(288, activation=\"relu\")(input_layer2)\n",
    "            dense1 = Dense(8, activation=\"relu\")(input_layer1)\n",
    "            flattened1 = Flatten()(dense1)\n",
    "            conv1 = Conv2D(filters=16, kernel_size=(2, 2), padding=\"same\", activation=\"relu\", kernel_regularizer=l1(0.0001))(input_layer1)\n",
    "            conv2 = Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", kernel_regularizer=l1(0.0001))(input_layer1)\n",
    "            pool1 = MaxPooling2D(pool_size=2, strides=1, padding='same')(conv1)\n",
    "            pool2 = MaxPooling2D(pool_size=2, strides=1, padding='same')(conv2)\n",
    "            flattened2 = Flatten()(pool1)\n",
    "            flattened3 = Flatten()(pool2)\n",
    "            conc = concatenate([flattened1, flattened2, flattened3, dense0])\n",
    "            dense2 = Dense(1024, activation=\"relu\", kernel_regularizer=l2(0.0001))(conc)\n",
    "            dense3 = Dense(256, activation=\"sigmoid\", kernel_regularizer=l2(0.0001))(dense2)\n",
    "            output = Dense(NUM_ACTIONS)(dense3)\n",
    "            reshape = Reshape((72,1))(output)\n",
    "            model = Model([input_layer1, input_layer2], reshape)\n",
    "            optimizer = Adam(0.0002)\n",
    "            model.compile(loss='mse', optimizer=optimizer, sample_weight_mode='temporal')\n",
    "            return model\n",
    "\n",
    "        def construct_q_network(self):\n",
    "\n",
    "            self.model = self.define_model()\n",
    "            self.target_model = self.define_model() #copy of the main model, every C iterations it gets the same weigth of the main model\n",
    "            self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "            print(\"Successfully constructed networks.\")\n",
    "\n",
    "\n",
    "        def predict_movement(self, data, randomChoice=False, greedy=False, action=-1):\n",
    "            \"\"\"\n",
    "            eps-greedy strategy, with probability eps it selects a random action.\n",
    "            With probability 1-eps it selects the action that maximize the estimated Q value for the state\n",
    "            \"\"\"\n",
    "            q_actions = self.model.predict(data)\n",
    "            opt_policy = np.argmax(q_actions[0,:,0])\n",
    "            \"\"\"q_actions = self.model.predict(data)\n",
    "            prob = np.exp(q_actions[0, :,0]) / np.sum(np.exp(q_actions[0, :,0]))\n",
    "            prob = np.nan_to_num(prob)\n",
    "            if np.sum(prob) < 1:\n",
    "                prob[0] += 1-np.sum(prob)\n",
    "            i = 0\n",
    "            while np.sum(prob) > 1:\n",
    "                prob[i] = max(0, prob[i]-(np.sum(prob)-1))\n",
    "                i += 1\n",
    "            opt_policy = np.random.choice(NUM_ACTIONS, 1, p=prob)[0]\"\"\"\n",
    "            if randomChoice:\n",
    "                #print(\"Random action\")\n",
    "                if np.random.random() < 0.1:\n",
    "                    opt_policy = np.random.randint(36, NUM_ACTIONS)\n",
    "                else:\n",
    "                    opt_policy = np.random.randint(0, 36)\n",
    "            if greedy:\n",
    "                #print(\"Greedy action\")\n",
    "                opt_policy = action\n",
    "            return opt_policy, q_actions[0, opt_policy, 0]\n",
    "        \n",
    "\n",
    "        def train(self, s_batch, a_batch, r_batch, s2_batch, epochs=1, batch_size=50):\n",
    "            \"\"\"\n",
    "            Training on a given batch.\n",
    "            See https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/\n",
    "            \"\"\"\n",
    "            batch_size = s_batch.shape[0]\n",
    "            targets = np.zeros((batch_size, NUM_ACTIONS,1))\n",
    "            s1 = []\n",
    "            s2 = []\n",
    "            sample_weights = np.ones((batch_size, NUM_ACTIONS))/(2*NUM_ACTIONS)\n",
    "            for i in range(batch_size):\n",
    "                s1.append(s_batch[i][0][0])\n",
    "                s2.append(s_batch[i][1][0])\n",
    "                targets[i] = self.target_model.predict([[s_batch[i][0][0]], [s_batch[i][1][0]]])\n",
    "                fut_action = self.model.predict([[s2_batch[i][0][0]], [s2_batch[i][1][0]]])\n",
    "                fut_action_index = np.argmax(fut_action)\n",
    "                target_fut_action = self.target_model.predict([[s2_batch[i][0][0]], [s2_batch[i][1][0]]])\n",
    "                targets[i, a_batch[i],0] = r_batch[i]\n",
    "                sample_weights[i, a_batch[i]] += 0.5\n",
    "                if i==0:\n",
    "                    print(\"Initial: \"+str(self.model.predict([[s_batch[0][0][0]], [s_batch[0][1][0]]])[0,a_batch[0],0])+\" \"+str(r_batch[0])+\" \"+str(target_fut_action[0,fut_action_index,0]))\n",
    "                if r_batch[i] > 0:\n",
    "                    targets[i, a_batch[i],0] += DECAY_RATE * target_fut_action[0,fut_action_index,0]\n",
    "                if r_batch[i] < 0:\n",
    "                    targets[i] = targets[i]*0\n",
    "            history = self.model.fit([s1,s2], targets, epochs=epochs, batch_size=batch_size, sample_weight=sample_weights, verbose=0)\n",
    "            print(\"Finale: \"+str(self.model.predict([[s_batch[0][0][0]], [s_batch[0][1][0]]])[0,a_batch[0],0]))\n",
    "            return history.history[\"loss\"][-1]\n",
    "\n",
    "\n",
    "        def save_network(self):\n",
    "            self.model.save(self.checkpoint_path)\n",
    "\n",
    "        def load_network(self):\n",
    "            self.model.load_weigths(self.checkpoint_path)\n",
    "\n",
    "        def target_train(self):\n",
    "            \"\"\"\n",
    "            Copy the weights of model in target_model.\n",
    "            \"\"\"\n",
    "            model_weights = self.model.get_weights()\n",
    "            target_model_weights = self.target_model.get_weights()\n",
    "            \"\"\"for i in range(len(model_weights)):\n",
    "                target_model_weights[i] = TAU * model_weights[i] + (1 - TAU) * target_model_weights[i]\"\"\"\n",
    "            self.target_model.set_weights(model_weights)\n",
    "        \n",
    "    BUFFER_SIZE = 400000 #experience replay buffer size\n",
    "    MINIBATCH_SIZE = 50\n",
    "    EPSILON_DECAY = 5000000\n",
    "    FINAL_EPSILON = 0.1\n",
    "    INITIAL_EPSILON = 0.40\n",
    "    C = 1024 #every C iterations, the weigths of model are copied into target_model\n",
    "\n",
    "    class DQLearner(GreedyLearner):\n",
    "\n",
    "        def __init__(self, env):\n",
    "            super().__init__(env)\n",
    "            self.replay_buffer = ReplayBuffer(BUFFER_SIZE)\n",
    "            self.network = DQLNetwork(env.num_tiles)\n",
    "            self.num_tiles = env.num_tiles\n",
    "            self.epsilon = INITIAL_EPSILON\n",
    "            self.last_reward = -1\n",
    "\n",
    "\n",
    "        def select_action(self):\n",
    "            \"\"\"\n",
    "            It selects an action in order to make a step in the environment. \n",
    "            The action is selected according to the predicted Q values for the current state by the model.\n",
    "            It returns the experience to add to the replay buffer.\n",
    "            \"\"\"\n",
    "            grid_state = np.array([to_categorical(self.grid, num_classes=self.num_tiles)])\n",
    "            storehouse_state = np.array([to_categorical(self.current_tile, num_classes=self.num_tiles).reshape(self.num_tiles)])\n",
    "            curr_state = [grid_state, storehouse_state]\n",
    "            if self.env.is_grid_full():\n",
    "                predict_movement, predict_q_value = self.network.predict_movement(curr_state, randomChoice=True)\n",
    "            else:\n",
    "                rand_val = np.random.random()\n",
    "                if rand_val < self.epsilon:\n",
    "                    if np.random.random() < 0.5:\n",
    "                        predict_movement, predict_q_value = self.network.predict_movement(curr_state, randomChoice=True)\n",
    "                    else:\n",
    "                        predict_movement, predict_q_value = self.network.predict_movement(curr_state, greedy=True, action=super().select_action())\n",
    "                else:\n",
    "                    if self.last_reward == 0:\n",
    "                        predict_movement, predict_q_value = self.network.predict_movement(curr_state, randomChoice=True)\n",
    "                    else:\n",
    "                        predict_movement, predict_q_value = self.network.predict_movement(curr_state)\n",
    "            print(\"Q_value :\"+str(predict_q_value))\n",
    "            action_type = \"place_current\"\n",
    "            position_on_the_grid = predict_movement\n",
    "            if position_on_the_grid >= 36:\n",
    "                action_type = \"place_from_storehouse\"\n",
    "                position_on_the_grid -= 36\n",
    "\n",
    "            reward, [self.grid, self.current_tile] = self.env.step(action_type, [int(position_on_the_grid/6), position_on_the_grid%6])\n",
    "            #if reward >= 0.1:\n",
    "            #    reward += (36-np.count_nonzero(self.grid))/36\n",
    "            new_grid_state = np.array([to_categorical(self.grid, num_classes=self.num_tiles)])\n",
    "            new_storehouse_state = np.array([to_categorical(self.current_tile, num_classes=self.num_tiles).reshape(self.num_tiles)])\n",
    "            new_curr_state = [new_grid_state, new_storehouse_state]\n",
    "            return curr_state, predict_movement, reward, new_curr_state\n",
    "        \n",
    "        def select_action_2(self):\n",
    "            \"\"\"\n",
    "            It selects an action in order to make a step in the environment. \n",
    "            The action is selected according to the predicted Q values for the current state by the model.\n",
    "            It returns the experience to add to the replay buffer.\n",
    "            \"\"\"\n",
    "            grid_state = np.array([to_categorical(self.grid, num_classes=self.num_tiles)])\n",
    "            storehouse_state = np.array(\n",
    "                [to_categorical(self.current_tile, num_classes=self.num_tiles).reshape(self.num_tiles)])\n",
    "            curr_state = [grid_state, storehouse_state]\n",
    "            q_actions = self.network.model.predict(curr_state)[0].reshape((72))\n",
    "            opt_policy_array = np.argsort(q_actions)[::-1]\n",
    "            i = 0\n",
    "            reward = 0\n",
    "            while reward == 0:\n",
    "                predict_movement = opt_policy_array[i]\n",
    "                predict_q_value = q_actions[predict_movement]\n",
    "                action_type = \"place_current\"\n",
    "                position_on_the_grid = predict_movement\n",
    "                if position_on_the_grid >= 36:\n",
    "                    action_type = \"place_from_storehouse\"\n",
    "                    position_on_the_grid -= 36\n",
    "\n",
    "                reward, [self.grid, self.current_tile] = self.env.step(action_type, [int(position_on_the_grid / 6),\n",
    "                                                                                 position_on_the_grid % 6])\n",
    "                i += 1\n",
    "            return reward\n",
    "\n",
    "\n",
    "        def simulate(self):\n",
    "            tot_reward = 0\n",
    "            num_experiments = 200\n",
    "            for experiment in range(num_experiments):\n",
    "                self.grid, self.current_tile = self.env.reset()\n",
    "                while self.env.is_grid_full() is not True:\n",
    "                    tot_reward += self.select_action_2()\n",
    "            self.grid, self.current_tile = self.env.reset()\n",
    "            print(\"Simulated reward: \"+str(tot_reward / num_experiments))\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        def train(self, epochs):\n",
    "            iterations = 0\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                if epoch % 10 == 0:\n",
    "                    self.simulate()\n",
    "                experiences = []\n",
    "                alive = True\n",
    "                total_reward = 0\n",
    "                pos_reward = 0\n",
    "                while alive:\n",
    "\n",
    "                    iterations += 1\n",
    "\n",
    "                    # Slowly decay the learning rate\n",
    "                    if self.epsilon > FINAL_EPSILON:\n",
    "                        self.epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EPSILON_DECAY\n",
    "\n",
    "                    initial_state, predicted_movement, reward, new_state = self.select_action()\n",
    "                    \n",
    "                    self.last_reward = reward\n",
    "\n",
    "                    total_reward += reward\n",
    "                    if reward > 0:\n",
    "                        pos_reward += reward\n",
    "                    print(\"Taken action: \" + str(predicted_movement) + \" tot_reward: \" + str(total_reward))\n",
    "                    \n",
    "\n",
    "                    if self.env.is_grid_full() and reward == -1:\n",
    "                        print(\"Earned a total of reward equal to \", total_reward)\n",
    "                        print(self.grid)\n",
    "                        self.grid, self.current_tile = self.env.reset()\n",
    "                        alive = False\n",
    "                        total_reward = 0\n",
    "                        pos_reward = 0\n",
    "                        self.last_reward = -1\n",
    "                        \n",
    "                    experiences.append([initial_state, predicted_movement, reward, new_state])   \n",
    "                    self.replay_buffer.add(initial_state, predicted_movement, reward, new_state)\n",
    "                    if iterations % 4 == 0:\n",
    "                        s_batch, a_batch, r_batch, s2_batch = self.replay_buffer.sample(MINIBATCH_SIZE)\n",
    "                        self.network.train(s_batch, a_batch, r_batch, s2_batch, epochs=1)\n",
    "\n",
    "                    if iterations % C == 0:\n",
    "                        self.network.target_train()\n",
    "                        print(\"epoch \"+str(epoch)+\" eps \"+str(self.epsilon)+\" reward \"+str(total_reward))\n",
    "                experiences = np.array(experiences)[::-1]\n",
    "                s_batch, a_batch, r_batch, s2_batch = experiences.T\n",
    "                loss = self.network.train(s_batch, a_batch, r_batch, s2_batch, epochs=1, batch_size=len(experiences))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    env = Env0()\n",
    "    dql_learner = DQLearner(env)\n",
    "    dql_learner.train(50000)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hops import experiment\n",
    "experiment.launch(keras_experiment, name='Keras classifier', local_logdir=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}